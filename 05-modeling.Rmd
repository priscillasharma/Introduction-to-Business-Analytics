# Modeling



```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = T, message = F, warnings = F)

```

In the last chapter  we cleaned the MegaTelCo data, removing rows with impossible or improbable values, as well as those with missing values.  We then explored the data looking for notable relationships between the target variable and various predictors.  We are now ready to model churn more formally.  The method we will use, as an illustration of modeling generally in the analytics lifecycle, is decision or classification trees, as implemented in the R package, `rpart`. (This is just one of many packages implementing decision trees in R.)  Before we get started, let's remind ourselves of the broader context of what we are doing.

This is a binary **classification problem** because churn has two possible outcomes---represented  in the target factor variable with the class labels, `LEAVE` and `STAY`.    And it is a **supervised problem** because we have historical observations of both leaving and staying, which can be used to  create a model of the relationship between various predictor variables--- demographic, behavioral, and what we have called attitudinal---and the target variable.  The problem can also be described as **supervised segmentation** because we are using that historical data to  create a model that will segment future customers into one of two groups: leave or stay.   (Remember:   if the outcome is continuous  then the problem is a regression problem, not a classification problem.) 

A secondary goal of the analytics project described in the Business Problem Statement was to identify customers most  likely to respond to a special re-enrollment offer.  However, the modeling demonstrated in this tutorial is solely focused on the first goal:  Create a model to predict which customers will churn.

## Data

First, load packages, including `rpart` and `rpart.plot` for fitting and visualizing tree models.  We will reproduce the cleaning from the previous chapter. Additionally, because our objective in this chapter is modeling, we will remove the ID variable, since, as an arbitrarily assigned number, it should have no predictive value. 

```{r}
#Load libraries
library(tidyverse)
library(rpart)
library(rpart.plot)

# Load the data
m <- read.csv("megatelco.csv")

# Clean the data
m_clean <- m %>% 
  mutate(reported_satisfaction = factor(reported_satisfaction,
                                        levels = c("low","avg", "high"),
                                        ordered = T),
         reported_usage_level = factor(reported_usage_level,
                                       levels = c("low",  "avg", "high"),
                                       ordered = T),
         considering_change_of_plan = factor(considering_change_of_plan,
                                       levels = c("no", "maybe", "yes")),
         leave = factor(leave),
         college = ifelse(college == "one", 1, 0)) %>% 
  filter(income > 0,
         house > 0,
         handset_price < 1000) %>% 
  na.omit %>% 
  select(-id)

# Inspect data
glimpse(m_clean)
```

As a reference, here is the data dictionary:

```{r}
data.frame(Variable = c("College", "Income",  "Overage", "Leftover", "House", "Handset_price", "Over_15mins_calls_per_month", "Average_call_duration", "Reported_satisfaction", "Reported_usage_level", "Considering_change_of_plan", "Leave","ID"),
            Explanation = c("Is the customer college educated?"," Annual income","Average overcharges per month","Average % leftover minutes per month","Estimated value of dwelling (from the census tract)", "Cost of phone", "Average number of long calls (15 mins or over) per month", "Average duration of a call", "Reported level of satisfaction", "Self-reported usage level", "Was customer considering changing his/her plan?","Class variable: whether customer left or stayed", "Numeric customer ID")) %>%
  kable
```


## Majority Class

It is important at the outset of classification modeling to determine the proportions of labels---the factor levels---in the target variable, in order to identify the majority class. In this case the target variable, representing customer churn, is `leave`.  What are the proportions of `LEAVE` and `STAY`? In the last chapter we saw how to to do that calculation:  

```{r proptable}
mean(m_clean$leave == "LEAVE") 
mean(m_clean$leave == "STAY") 
```

Suppose that we would like to predict whether a new customer will churn. One option, without any further modeling, would be to use the majority class in the target variable of the historical data as the prediction. This would not be a very good model.  Based on the historical data predictions from this majority class model would be correct about 51% of the time. But that would be *slightly* better than random guessing. Any model we create should therefore have better performance than the majority class model.

## Tree Model

We will be using the `rpart` package in R, an implementation of the [CART algorithm](https://en.wikipedia.org/wiki/Decision_tree_learning), to fit a classification tree model to `leave` in the MegaTelCo data. (`rpart` stands for "Recursive Partitioning and Regression Trees"; see the [package vignette](https://cran.r-project.org/web/packages/rpart/vignettes/longintro.pdf) for implementation details.)  The model syntax used in this package is typical for R, consisting in a function, `rpart()`, with formula and data arguments.  Formulas in R typically take the form of `y ~ x + z`, with the `y` standing for the variable to be modeled, the tilde indicating "modeled by" or "explained by," and the predictors, here `x` and `z`, included additively.  

Let's demonstrate how to use  `rpart` for classification by fitting a simple tree---call it a stump---of `leave` with just one predictor, `house`, which we noticed in  the previous tutorial seemed somewhat correlated with the target variable. As a reminder, here is the plot: 

```{r boxplot}

ggplot(data = m_clean, aes(x = leave, y = house)) +
  geom_boxplot() +
  labs(title = "leave ~ house")
  
```

Here is the tree:

```{r tree, echo=T}
# Fit tree
(tree_model <- rpart(leave ~ house, data = m_clean))

```

I have assigned the model to an object, `tree_model`, which we will use later.  For now, let's work through these simplified results. One thing to keep in mind is that due to the factor structure in the target variable---due to the alphabetic defaults for the `factor()` function, the first level is `LEAVE` and the second level is `STAY`---the algorithm is specifically modeling `STAY`

- `n =`: n is the number of rows in the data set: 4994.
- `1) root`:  The root node by definition has no split, and consists in the entire dataset: 4994 observations.  The next number, 2468, referred to as "loss" in this output, is the count of misclassifications that would occur if the majority class label in the node, `STAY`, were used for prediction. The proportions of `LEAVE` and `STAY` in the data are .49 and .51, respectively.
- `2) house < 600255.5`: the first split is on `house` at 600,225.5, with the left branch (`house< 600,255.5`) producing a group of size 3306, with loss of 1364 incorrectly classified observations.  The class label prediction for this node is the majority label, `LEAVE`, accounting for almost 59% of the observations in the node. This is a leaf node, as indicated by the asterisk.
- `3) house >= 600255.5`: the right branch produces a group of 1688, with loss of 526.  The class label prediction for this node is the majority label, `STAY`, accounting for almost 69% of the observations in this leaf node.

A classification tree can be visualized as an upside down tree with the "root" at the top and "nodes" at each split or decision point, with the "leaves" at the terminal nodes. Here is a visualization of the tree using the `rpart.plot` function:

```{r tree-plot}
# Plot tree object
rpart.plot(tree_model)

```



How to read this?  66% of the data (3306) is in the left branch and 34% (1688) in the right. The majority class in the left leaf node is `LEAVE`, with 1 - .41 = .59 or 59% of the observations, which is therefore the prediction for all observations falling in this node.  The reason the tree lists the proportion of `STAY` in the left branch is because, as noted above, that is the category we are explicitly modeling due to the variable's factor structure. The majority class, and hence the predicted class for that group, in the right node is `STAY` with 69% of the observations.

So, a tree model is super simple to set up in R, though the output can be tricky to interpret.  It helps to have an example handy when interpreting a tree.

Notice that, while `rpart()` has created a model of `STAY`, the model provides information about both `STAY` and `LEAVE`, telling us when---according to which values of `house`---to predict one and when to predict the other. 

I'm using the word "predict" here in a possibly misleading way, since this model is not predicting, in the sense of *forecasting* whether or not a customer will churn in the future, but rather *describing* when a customer *has* churned.  The model is simply describing patterns in the existing data.  However, the aim of creating such a model is to use it eventually for prediction, so we will loosely refer to the model as "predicting" the outcome in this case even when we already know the outcome.

Let's use this simple tree model to review the concepts of purity, entropy and information gain.

## Purity and Entropy

Purity is a commonsense notion.  The leaf nodes in a tree model are pure if they all have the same class label.  If they don't---as in our simple model---then they are impure.  For example, the observed outcomes in the leaf node in the left branch consist in 41% `STAY` and 59% `LEAVE`, and, in the right branch, 69% `STAY` and 31% `LEAVE`.  These leaf nodes are impure, then, but how impure? Entropy answers that question for us, formalizing the notion of impurity.   Entropy ranges between 0 and 1, with 0 indicating  no  disorder (the group is pure)  and 1 indicating maximal disorder (the group is balanced).

Figure 3-3 in DSB is helpful:

![](images/entropy.png)

In our our tree model, entropy would be 0 if the leaf nodes were either all `LEAVE` or all `STAY`.  The nodes are mixed though, and as the mixture of these two classes increases, so does entropy.  If the two classes were balanced 50-50 within a node (at the top of the curve in 3-3) then the node would have maximum disorder and entropy would be 1. 

Entropy is  formally defined as: $-p_1 log(p_1) - p_2 log(p_2) ...$

We will follow DSB in using log base 2 to calculate entropy and information gain.  

Entropy in the two leaf nodes would be:

- Left leaf node:  -.41 x log(.41) - .59 x log(.59) = .98.
- Right leaf node:  -.69 x log(.69) - .31 x log(.31) = .89.

This result makes intuitive sense since entropy is higher in the more impure left node where the classes are more evenly balanced. 

## Information Gain

Entropy thus inversely measures the purity of a node: as purity goes down entropy goes up.  It is a node-level metric.  Information gain (IG), by contrast, measures the *change in node purity* resulting from a split. The goal in a creating a classification tree is, of course, to find splits that make nodes with the least possible entropy or disorder (ideally, nodes would be pure, with all of the observations having the same class label). After a split the two nodes (known as children) should together have higher purity (lower entropy) than the original node before the split (known as the parent). We use IG comparatively to evaluate possible splits with the goal of identifying the one split out of many candidates that most decreases entropy (increases purity) in the children relative to the parent. The concept is fairly simple, though the formula looks ferocious and is tricky to use: 


$$IG(parent, children) = entropy(parent) - [p(c_1) entropy(c_1) + p(c_2) entropy(c_2) + ...]$$

The formula combines the weighted entropy in the children (weighted by the proportion of the data in each node) and subtracts it from the entropy in the parent.

The $IG$ calculation or the tree model would be:

- $entropy(parent)$ uses the proportion of `LEAVE` and `STAY` in the parent node, which in this case is the entire dataset: -.49 x log(.49) - .51 x log(.51) = .9997.
- $p(c_1)$  is the proportion of observations  in the left branch node: 3306/4994 = .66
- $p(c_2)$ is the proportion of observations in the right branch node: 1 - .66 = .34
- $entropy(c_1)$ from above: -.41 x log(.41) - .59 x log(.59) = .98.
- $entropy(c_2)$ from above: -.69 x log(.69) - .31 x log(.31) = .89.

$IG$ = .997 - (.66 x .98 + .34 x ..89) = .05.

This calculation is a little off due to rounding error.  Below is a plot showing the exact $IG$ associated with each potential split on house. The value of `house` at maximum $IG$ is about 600,000 (black dashed line) as identified by the tree model.

```{r echo = F}
# Create a data.frame of house prices from min to max by 1000
# and an empty column for storing the corresponding entropy
# df <- data.frame(house = seq(min(m_clean$house) + 1000, max(m_clean$house), by = 500),
#                  entropy = 0,
#                  info_gain = 0)
# 
# # Check the data frame
# # head(df)
# 
# # parent entropy
# parent_entropy <- m_clean %>%
#   dplyr::summarize(p1 = mean(ifelse(leave=="STAY", 1, 0)),
#             p2 = 1-p1,
#             entropy = -p1*log2(p1)-p2*log2(p2)) %>%
#   dplyr::select(entropy) %>% 
#   as.numeric
# 
# # Set up a loop to compute entropies
# for(i in seq_along(df$house)){
#   temp <- m_clean %>%
#     mutate(predict = ifelse(house > df$house[i], "LEAVE", "STAY"),
#            leave = leave) %>%
#     count(predict, leave) %>%
#     group_by(predict) %>%
#     mutate(perc = n/sum(n),
#            entropy = -(first(perc)* log2(first(perc)) + last(perc)* log2(last(perc)))) %>% 
#     group_by(predict) %>% 
#     mutate(total = sum(n)) %>%
#     slice(1) %>% 
#     dplyr::select(predict, entropy, total) %>% 
#     ungroup %>% 
#     mutate(perc=total/sum(total)) 
#   
#   df$entropy[i] <- -(temp$perc[1] * log2(temp$perc[1]) + temp$perc[2] * log2(temp$perc[2]))
#     
#   df$info_gain[i] <- parent_entropy - (temp$perc[1] * temp$entropy[1] +
#                                                temp$perc[2] * temp$entropy[2])
# }
#   
# 
# 
# names(df)[3] <- "Information gain"
# 
# write_csv(df, "ig_table.csv")


df <- read_csv("ig_table.csv")

ggplot(df, aes(house, `Information gain`)) +
  geom_line() +
  geom_vline(xintercept = as.integer(df[which(df$`Information gain`==max(df$`Information gain`)), "house"]), lty=2, col = 2)+
  labs(title="Information gain at different splits on house",
       subtitle="Max IG in red",
       y = "information gain") + 
  theme_minimal()

```


## Greediness

The classification tree algorithm does an efficient search for the best split among all the available predictors---the split that most increases IG.  The search is "greedy" in the sense that the algorithm looks for the best split conditional on all the previous splits. This makes the splits locally optimal, with no guarantee of being globally optimal.

>A greedy algorithm is an algorithmic paradigm that follows the problem solving heuristic of making the locally optimal choice at each stage with the intent of finding a global optimum. In many problems, a greedy strategy does not usually produce an optimal solution, but nonetheless a greedy heuristic may yield locally optimal solutions that approximate a globally optimal solution in a reasonable amount of time. [Wikipedia](https://en.wikipedia.org/wiki/Greedy_algorithm)

In other words, a greedy algorithm always chooses the best step at a given point, which is not necessarily (though probably gets close to) the best step overall.

The tree is finished when the algorithm can find no additional information gain-improving splits.  The resulting "leaf" nodes ideally define subgroups or segments within the data that have low entropy.  The majority class in each leaf node will be used as the model's prediction for observations sharing those characteristics in new data when the outcome is unknown.


## Model Accuracy

Is this a good model?  How would we know?  One of the most common measures of classification model performance is accuracy, defined as the proportion of correct classifications (hence ranging from 0 to 1). We can calculate accuracy using the the model output:

```{r tree_mod}
tree_model
```

The left branch node has 3306 observations, 1364 of which were incorrect.  The right branch node has 1688 observations, 526 of which were incorrect.  Accuracy will thus be:  ((3306 - 1364) + (1688 - 526)) / 4994 = .62.  62% of the model's classifications were correct, and 1 - .62 = .38 is the proportion of *incorrect* classifications.

This number can be obtained more simply using the `predict()` function.  The `predict()` function extracts from the model object the class label prediction for each row:


```{r predict_tree}
predict(tree_model, type = "class") %>% 
  head
```

Using this output we can calculate accuracy as follows:


```{r acc}
(predict(tree_model, type = "class") == m_clean$leave) %>% 
  mean
```

This code works because the double equals sign, "==," evaluates to `TRUE` or `FALSE` (in effect it asks: does the predicted value equal the observed value, T or F?), and in R the `mean()` function counts `TRUE` as 1.  The code sums the instances where predicted is identical to observed and divides by the number of rows.



## Model Improvement

Can we improve the model?   Let's fit a tree model with all the  available predictors . We will use shorthand notation--- "~ ."---for including all the predictors additively: 

```{r}
# Fit tree
(tree_model2 <- rpart(leave ~ ., data = m_clean))


# Visualize
rpart.plot(tree_model2)
```

Plots with lots of nodes and branches can become too complicated to view. But we can adjust the `tweak` argument and the `roundint` argument to the plotting function to make a more legible plot. Play around with different numbers. `tweak = 1` will produce the same plot as above.

```{r}
rpart.plot(tree_model2, tweak = 1, roundint=T)
```

This plot is different---indeed the model is different---from the one displayed in Figure 3-18 of DSB.  That is presumably because of differences in software implementation.  Notice in the model summary that there are missing nodes---the count jumps from 13 to 22.  The missing nodes have been "pruned" by the algorithm after growing the tree because they did not improve model performance.


How would we interpret this model? Notice that the very top of the tree gives a key: observations are classified into the right branch when the condition stated in the node is false ("no") and classified into the left branch when true ("yes"). Take the rightmost leaf in the tree as an example.  If house > 600k and income is less than 100k then the model predicts `STAY`. 


Has the addition of predictors improved the model's accuracy?


```{r}
# Evaluate accuracy
(predict(tree_model2, type = "class") == m_clean$leave)  %>% 
  mean

```

The model's accuracy has improved to .71. According to the accuracy metric this is a better model than the one with a single predictor, `house`.

What does this mean?  Well, in terms of a marketing campaign, we should be realistic about the fact that something like 30% of the customers predicted to churn, and to whom we might extend a special retention offer, will have been incorrectly classified.  (The number might actually be higher than that; we will ignore the possibility for now that the model might be overfitting.) We need to factor this error--- and uncertainty about how large the error is likely to be in practice---into any recommended approach to improving customer retention at MegaTelCo.

## Variable Importance

We can get a sense of which variables are most important in predicting churn by looking at the tree plot:  the splitting variables closer to the root node are more important. Clearly `house` is the strongest predictor, followed by `overage` and `income`.  This can be useful when doing explanatory or descriptive modeling in order to understand which factors contribute to churn.  However, it is important to understand that variable importance does not imply causation.  For example, we would not want to say that certain housing values cause churn!  Notice that the surprising finding from EDA was confirmed by the tree model:  customer satisfaction is not a predictor of churn.

